{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-D98CddQuwKG"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "I8rGMlN2uzgk"
      },
      "outputs": [],
      "source": [
        "ENVIRONMENT = \"ALE/Pong-v5\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SAVE_MODELS = True  # 保存文件\n",
        "MODEL_PATH = \"./pong-cnn-\"  # 文件路径\n",
        "SAVE_MODEL_INTERVAL = 10  # 每多少次保存一次模型\n",
        "TRAIN_MODEL = True  # 游戏过程中训练，测试时设为False\n",
        "\n",
        "LOAD_MODEL_FROM_FILE = True  # 加载模型\n",
        "LOAD_FILE_EPISODE = 430  # 加载模型的轮数\n",
        "\n",
        "BATCH_SIZE = 64  # Minibatch size\n",
        "MAX_EPISODE = 100000  # 最大幕数\n",
        "MAX_STEP = 100000  # 一幕中最大步数\n",
        "\n",
        "MAX_MEMORY_LEN = 50000  \n",
        "MIN_MEMORY_LEN = 40000 \n",
        "\n",
        "GAMMA = 0.97  # 折扣率\n",
        "ALPHA = 0.00025  # 学习步长\n",
        "EPSILON_DECAY = 0.99  # Epsilon衰减率\n",
        "\n",
        "RENDER_GAME_WINDOW = False  # 渲染窗口"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "HxF5-bzUu1q-"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, h, w, output_size):\n",
        "        # 网络结构\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=32, kernel_size=8, stride=4)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        convw, convh = self.conv2d_size_calc(w, h, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        convw, convh = self.conv2d_size_calc(convw, convh, kernel_size=3, stride=1)\n",
        "\n",
        "        linear_input_size = convw * convh * 64\n",
        "\n",
        "        # 行为\n",
        "        self.Alinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Alrelu = nn.LeakyReLU()\n",
        "        self.Alinear2 = nn.Linear(in_features=128, out_features=output_size)\n",
        "\n",
        "        # 状态\n",
        "        self.Vlinear1 = nn.Linear(in_features=linear_input_size, out_features=128)\n",
        "        self.Vlrelu = nn.LeakyReLU() \n",
        "        self.Vlinear2 = nn.Linear(in_features=128, out_features=1) \n",
        "\n",
        "    def conv2d_size_calc(self, w, h, kernel_size=5, stride=2):\n",
        "        # 计算卷积层输出图像大小\n",
        "        next_w = (w - (kernel_size - 1) - 1) // stride + 1\n",
        "        next_h = (h - (kernel_size - 1) - 1) // stride + 1\n",
        "        return next_w, next_h\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # 展平\n",
        "\n",
        "        Ax = self.Alrelu(self.Alinear1(x))\n",
        "        Ax = self.Alinear2(Ax)  # 没有激活\n",
        "\n",
        "        Vx = self.Vlrelu(self.Vlinear1(x))\n",
        "        Vx = self.Vlinear2(Vx)  # 没有激活\n",
        "\n",
        "        q = Vx + (Ax - Ax.mean())\n",
        "\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "plT51MPbu5U5"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, environment):\n",
        "        # 智能体超参数\n",
        "        # 环境图像大小，作为状态\n",
        "        self.state_size_h = environment.observation_space.shape[0]\n",
        "        self.state_size_w = environment.observation_space.shape[1]\n",
        "        self.state_size_c = environment.observation_space.shape[2]\n",
        "\n",
        "        # 动作大小\n",
        "        self.action_size = environment.action_space.n\n",
        "\n",
        "        # 与处理后的参数\n",
        "        self.target_h = 80\n",
        "        self.target_w = 64\n",
        "\n",
        "        self.crop_dim = [20, self.state_size_h, 0, self.state_size_w]\n",
        "\n",
        "        \n",
        "        self.gamma = GAMMA  # 折扣率\n",
        "        self.alpha = ALPHA  # 学习率\n",
        "\n",
        "        self.epsilon = 1  # Epsilon的初始值\n",
        "        self.epsilon_decay = EPSILON_DECAY  # 衰减率\n",
        "        self.epsilon_minimum = 0.05  # Epsilon的最小值\n",
        "\n",
        "        # 用于经验回放\n",
        "        self.memory = deque(maxlen=MAX_MEMORY_LEN)\n",
        "\n",
        "        # 定义模型\n",
        "        self.online_model = CNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model = CNN(h=self.target_h, w=self.target_w, output_size=self.action_size).to(DEVICE)\n",
        "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
        "        self.target_model.eval()\n",
        "\n",
        "        # 设置优化器\n",
        "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=self.alpha)\n",
        "\n",
        "    def preProcess(self, image):\n",
        "        # 图像预处理\n",
        "        frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # 灰度图\n",
        "        frame = frame[self.crop_dim[0]:self.crop_dim[1], self.crop_dim[2]:self.crop_dim[3]]\n",
        "        frame = cv2.resize(frame, (self.target_w, self.target_h)) \n",
        "        frame = frame.reshape(self.target_w, self.target_h) / 255  # 归一化\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def act(self, state):\n",
        "        # 选择动作\n",
        "        act_protocol = 'Explore' if random.uniform(0, 1) <= self.epsilon else 'Exploit'\n",
        "\n",
        "        if act_protocol == 'Explore':   # 随机选择动作\n",
        "            action = random.randrange(self.action_size)\n",
        "        else:\n",
        "            with torch.no_grad():   # 按照神经网络选择动作\n",
        "                state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
        "                q_values = self.online_model.forward(state) \n",
        "                action = torch.argmax(q_values).item() \n",
        "\n",
        "        return action\n",
        "\n",
        "    def train(self):\n",
        "        # 训练神经网络\n",
        "        if len(agent.memory) < MIN_MEMORY_LEN:\n",
        "            loss, max_q = [0, 0]\n",
        "            return loss, max_q\n",
        "        \n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.memory, BATCH_SIZE))\n",
        "\n",
        "        state = np.concatenate(state)\n",
        "        next_state = np.concatenate(next_state)\n",
        "\n",
        "        # 转换为张量\n",
        "        state = torch.tensor(state, dtype=torch.float, device=DEVICE)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float, device=DEVICE)\n",
        "        action = torch.tensor(action, dtype=torch.long, device=DEVICE)\n",
        "        reward = torch.tensor(reward, dtype=torch.float, device=DEVICE)\n",
        "        done = torch.tensor(done, dtype=torch.float, device=DEVICE)\n",
        "\n",
        "        # 预测\n",
        "        state_q_values = self.online_model(state)\n",
        "        next_states_q_values = self.online_model(next_state)\n",
        "        next_states_target_q_values = self.target_model(next_state)\n",
        "\n",
        "        # 获取动作值函数\n",
        "        selected_q_value = state_q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        next_states_target_q_value = next_states_target_q_values.gather(1, next_states_q_values.max(1)[1].unsqueeze(1)).squeeze(1)\n",
        "        expected_q_value = reward + self.gamma * next_states_target_q_value * (1 - done)    # 贝尔曼方程\n",
        "\n",
        "        # 计算损失\n",
        "        loss = (selected_q_value - expected_q_value.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, torch.max(state_q_values).item()\n",
        "\n",
        "    def storeResults(self, state, action, reward, nextState, done):\n",
        "        # 存储经验\n",
        "        self.memory.append([state[None, :], action, reward, nextState[None, :], done])\n",
        "\n",
        "    def adaptiveEpsilon(self):\n",
        "        # Epsilon衰减\n",
        "        if self.epsilon > self.epsilon_minimum:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ve4vYDe3bozg"
      },
      "outputs": [],
      "source": [
        "# 设置环境\n",
        "environment = gym.make(ENVIRONMENT, render_mode='rgb_array')\n",
        "agent = Agent(environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if LOAD_MODEL_FROM_FILE:\n",
        "    agent.online_model.load_state_dict(torch.load(MODEL_PATH+str(LOAD_FILE_EPISODE)+\".pkl\"))\n",
        "\n",
        "    with open(MODEL_PATH+str(LOAD_FILE_EPISODE)+'.json') as outfile:\n",
        "        param = json.load(outfile)\n",
        "        agent.epsilon = param.get('epsilon')\n",
        "\n",
        "    startEpisode = LOAD_FILE_EPISODE + 1\n",
        "\n",
        "else:\n",
        "    startEpisode = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "last_100_ep_reward = deque(maxlen=100)  # 近100轮的奖励\n",
        "total_step = 1  # 累计步数\n",
        "for episode in range(startEpisode, MAX_EPISODE):\n",
        "\n",
        "    startTime = time.time()\n",
        "    state = environment.reset()[0]  # 重置环境\n",
        "\n",
        "    state = agent.preProcess(state)  # 图像预处理\n",
        "    state = np.stack((state, state, state, state))  # 堆叠连续的4帧图像\n",
        "\n",
        "    total_max_q_val = 0\n",
        "    total_reward = 0\n",
        "    total_loss = 0\n",
        "    for step in range(MAX_STEP):\n",
        "\n",
        "        if RENDER_GAME_WINDOW:\n",
        "            environment.render()  # 渲染图像\n",
        "\n",
        "        # 选择动作\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, truncated, info = environment.step(action)  # 下一状态\n",
        "\n",
        "        next_state = agent.preProcess(next_state)  # 预处理\n",
        "        next_state = np.stack((next_state, state[0], state[1], state[2]))   # 连续4帧\n",
        "\n",
        "        # 存储经验\n",
        "        agent.storeResults(state, action, reward, next_state, done)\n",
        "      \n",
        "        state = next_state\n",
        "\n",
        "        if TRAIN_MODEL:\n",
        "            loss, max_q_val = agent.train() # 训练模型\n",
        "        else:\n",
        "            loss, max_q_val = [0, 0]\n",
        "\n",
        "        total_loss += loss\n",
        "        total_max_q_val += max_q_val\n",
        "        total_reward += reward\n",
        "        total_step += 1\n",
        "        if total_step % 1000 == 0:\n",
        "            agent.adaptiveEpsilon()\n",
        "\n",
        "        if done:\n",
        "            currentTime = time.time()\n",
        "            time_passed = currentTime - startTime \n",
        "            current_time_format = time.strftime(\"%H:%M:%S\", time.gmtime())\n",
        "            epsilonDict = {'epsilon': agent.epsilon} \n",
        "\n",
        "            if SAVE_MODELS and episode % SAVE_MODEL_INTERVAL == 0:  # 保存模型\n",
        "                weightsPath = MODEL_PATH + str(episode) + '.pkl'\n",
        "                epsilonPath = MODEL_PATH + str(episode) + '.json'\n",
        "\n",
        "                torch.save(agent.online_model.state_dict(), weightsPath)\n",
        "                with open(epsilonPath, 'w') as outfile:\n",
        "                    json.dump(epsilonDict, outfile)\n",
        "\n",
        "            if TRAIN_MODEL:\n",
        "                agent.target_model.load_state_dict(agent.online_model.state_dict()) \n",
        "\n",
        "            last_100_ep_reward.append(total_reward)\n",
        "            avg_max_q_val = total_max_q_val / step\n",
        "\n",
        "            outStr = \"Episode:{} Time:{} Reward:{:.2f} Loss:{:.2f} Last_100_Avg_Rew:{:.3f} Avg_Max_Q:{:.3f} Epsilon:{:.2f} Duration:{:.2f} Step:{} CStep:{}\".format(\n",
        "                episode, current_time_format, total_reward, total_loss, np.mean(last_100_ep_reward), avg_max_q_val, agent.epsilon, time_passed, step, total_step\n",
        "            )\n",
        "\n",
        "            print(outStr)\n",
        "\n",
        "            if SAVE_MODELS:\n",
        "                outputPath = MODEL_PATH + \"out\" + '.txt'\n",
        "                with open(outputPath, 'a') as outfile:\n",
        "                    outfile.write(outStr+\"\\n\")\n",
        "\n",
        "            break\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "OpenAIPong-DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
