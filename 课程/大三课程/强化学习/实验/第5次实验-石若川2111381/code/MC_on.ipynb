{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class On_policy_MC:\n",
    "    def __init__(self, env, start_state, gamma=0.8):\n",
    "        #初始化行为值函数\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.action_size = env.action_space.n\n",
    "        self.qvalue = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        #初始化每个状态-动作对的次数\n",
    "        self.n = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        self.states = []    # 状态空间500\n",
    "        for i in range(self.state_size):\n",
    "            self.states.append(i)\n",
    "\n",
    "        self.actions = []   # 动作空间6\n",
    "        for i in range(self.action_size):\n",
    "            self.actions.append(i)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 0.5\n",
    "        self.env = env\n",
    "        self.start_state = start_state\n",
    "\n",
    "        #初始化策略\n",
    "        self.Pi = (1/self.action_size) * np.ones((self.state_size, self.action_size))\n",
    "        self.Greedy_Pi = np.zeros((self.state_size,self.action_size))\n",
    "        self.cur_state = 0\n",
    "        self.cur_action = 0\n",
    "        self.old_policy = np.ones((self.state_size, self.action_size))\n",
    "        #################状态转移概率P(s'|s,a)与回报模型模型构建#################################\n",
    "        self.P_ssa = np.zeros((self.action_size, self.state_size, self.state_size))\n",
    "        self.r_sa = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        for i in range(self.state_size):\n",
    "            for j in range(self.action_size):\n",
    "                next = self.env.P[i][j]\n",
    "                for k in range(len(next)):\n",
    "                    probability, next_state, reward, done = next[k]\n",
    "                    self.P_ssa[j][i][next_state] += probability\n",
    "                    self.r_sa[i][j] += probability*(reward)\n",
    "                    # if reward == -1:\n",
    "                    #     self.r_sa[i][j] += probability*(-0.1)\n",
    "                    # elif reward == -10:\n",
    "                    #     self.r_sa[i][j] += probability*(-1)\n",
    "                    # else:\n",
    "                    #     self.r_sa[i][j] += probability*(1000)\n",
    "                    \n",
    "        \n",
    "    #重置环境函数\n",
    "    def reset(self):\n",
    "        # 初始化行为值函数\n",
    "        self.qvalue = np.zeros((self.state_size, self.action_size))\n",
    "        # 初始化每个状态-动作对的次数\n",
    "        self.n = np.zeros((self.state_size, self.action_size))\n",
    "    #探索初始化函数\n",
    "    def explore_init(self):\n",
    "        state_prob = (1/self.state_size)*np.ones((1, self.state_size))\n",
    "        s0 = np.random.choice(self.states,p=state_prob)\n",
    "        action_prob = (1/self.action_size)*np.ones((1, self.action_size))\n",
    "        a0 = np.random.choice(self.actions,p=action_prob)\n",
    "        return s0,a0\n",
    "    #根据策略pi采样一个动作\n",
    "    def sample_action(self,state):\n",
    "        action = np.random.choice(self.actions,p=self.Pi[state,:])\n",
    "        # print(\"random action: \",action)\n",
    "        return action\n",
    "    #跟环境交互一步\n",
    "    def step(self,action):\n",
    "        probability, next_state, r_next, done = self.env.P[self.cur_state][action][0]\n",
    "        # if r_next == -1:\n",
    "        #     r_next = -0.1\n",
    "        # elif r_next == -10:\n",
    "        #     r_next = -1\n",
    "        # else:\n",
    "        #     r_next = 1000\n",
    "\n",
    "        return next_state,r_next,done\n",
    "    #############策略改进源代码##########\n",
    "    def update_policy(self):\n",
    "        for i in range(self.state_size):\n",
    "            self.Pi[i,:]=0\n",
    "            max_num = np.argmax(self.qvalue[i,:])\n",
    "            self.Pi[i, max_num] = 1\n",
    "    def get_greedy_policy(self):\n",
    "        for i in range(self.state_size):\n",
    "            self.Greedy_Pi[i,:]=0\n",
    "            max_num = np.argmax(self.qvalue[i, :])\n",
    "            self.Greedy_Pi[i, max_num] = 1\n",
    "        return self.Greedy_Pi\n",
    "\n",
    "    def update_epsilon_greedy(self):\n",
    "        for i in range(self.state_size):\n",
    "            self.Pi[i,:]=self.epsilon/self.action_size\n",
    "            max_num = np.argmax(self.qvalue[i,:])\n",
    "            self.Pi[i, max_num] = self.epsilon/self.action_size+(1-self.epsilon)\n",
    "\n",
    "\n",
    "    #蒙特卡洛强化学习算法\n",
    "    def MC_learning(self):\n",
    "        num = 0\n",
    "        while num<int(1e5):\n",
    "            num+=1\n",
    "            flag=False\n",
    "            #采样一条轨迹\n",
    "            state_traj=[]\n",
    "            action_traj = []\n",
    "            reward_traj=[]\n",
    "            g = 0\n",
    "            episode_num = 0\n",
    "            # 从初始状态出发\n",
    "            self.cur_state = self.start_state\n",
    "            while flag==False and episode_num<100:\n",
    "                #与环境交互一次\n",
    "                cur_action = self.sample_action(self.cur_state)\n",
    "                # print(\"cur_action: \",cur_action)\n",
    "                state_traj.append(self.cur_state)\n",
    "                action_traj.append(cur_action)\n",
    "                next_state, reward, flag = self.step(cur_action)\n",
    "                # print('reward:', reward)\n",
    "                reward_traj.append(reward)\n",
    "                self.cur_state = next_state\n",
    "                episode_num += 1\n",
    "            print(\"state_traj\",state_traj)\n",
    "            # print(\"reward_traj\",reward_traj)\n",
    "            ############利用采集到的轨迹更新行为值函数################\n",
    "            for i in reversed(range(len(state_traj))):\n",
    "                #计算状态-动作对(s,a)的访问频次\n",
    "                self.n[state_traj[i],action_traj[i]]+=1.0\n",
    "                #利用增量式方式更新当前状态动作值\n",
    "                g*=self.gamma\n",
    "                g+=reward_traj[i]\n",
    "                self.qvalue[state_traj[i],action_traj[i]]=\\\n",
    "                    (self.qvalue[state_traj[i], action_traj[i]]*(self.n[state_traj[i],action_traj[i]]-1)+g)/ \\\n",
    "                    self.n[state_traj[i], action_traj[i]]\n",
    "            # if state_traj[0] == 1 and action_traj[0] == 3:\n",
    "            #     print(\"state_traj\", state_traj)\n",
    "            #     print(\"状态频次及值函数\", self.n[1, 3],self.qvalue[1,3] )\n",
    "            ###########更新策略################\n",
    "            if num%200==0:\n",
    "                self.old_policy = copy.deepcopy(self.Pi)\n",
    "                self.update_epsilon_greedy()\n",
    "                self.epsilon = self.epsilon*0.99\n",
    "                self.n = np.zeros((self.state_size,self.action_size))\n",
    "                # self.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_ana_evaluate(Pi,r_sa,P_ssa,gamma):\n",
    "    state_size = 500\n",
    "    action_size = 6\n",
    "    # state_size = 16\n",
    "    # action_size = 4\n",
    "    P_pi = np.zeros((state_size, state_size))\n",
    "    C_pi = np.zeros((state_size, 1))\n",
    "\n",
    "    for i in range(state_size):\n",
    "        # 计算pi(a|s)*p(s'|s,a)\n",
    "        P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()\n",
    "        # 计算pi(a|s)*r(s,a)\n",
    "        C_pi[i, :] = np.dot(r_sa[i, :], Pi[i, :])\n",
    "\n",
    "    ############解析法计算值函数######################    \n",
    "    # # 找出全零行\n",
    "    # zero_rows = np.where(~P_pi.any(axis=1))[0]\n",
    "\n",
    "    # # 找出全零列\n",
    "    # zero_columns = np.where(~P_pi.any(axis=0))[0]\n",
    "\n",
    "    # print(\"全零行的索引：\", zero_rows)\n",
    "    # print(\"全零列的索引：\", zero_columns)\n",
    "\n",
    "    # print(\"det of P_pi\", np.linalg.det(P_pi))\n",
    "\n",
    "    M = np.eye(state_size) - P_pi\n",
    "    \n",
    "    M_det = np.linalg.det(M)\n",
    "    print(\"det(M)=\", M_det)\n",
    "\n",
    "    # 找出全零行\n",
    "    zero_rows = np.where(~M.any(axis=1))[0]\n",
    "\n",
    "    # 找出全零列\n",
    "    zero_columns = np.where(~M.any(axis=0))[0]\n",
    "\n",
    "    print(\"全零行的索引：\", zero_rows)\n",
    "    print(\"全零列的索引：\", zero_columns)\n",
    "\n",
    "    I_M = np.linalg.inv(M)\n",
    "    V = np.dot(I_M, C_pi)\n",
    "    #计算行为值函数\n",
    "    q_value = np.zeros((state_size, action_size))\n",
    "    for i in range(state_size):\n",
    "        q_sa = np.zeros((1, action_size))\n",
    "        for j in range(action_size):\n",
    "            Pi[i, :] = 0\n",
    "            Pi[i, j] = 1\n",
    "            P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()\n",
    "            vi = np.dot(r_sa[i, :], Pi[i, :]) + np.dot(P_pi[i, :], V.squeeze())\n",
    "            q_sa[0, j] = vi\n",
    "        q_value[i, :] = q_sa[0, :]\n",
    "    return q_value\n",
    "\n",
    "# q_real_value = q_ana_evaluate(on_policy_MC.Pi,on_policy_MC.r_sa,on_policy_MC.P_ssa, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"rgb_array_list\")\n",
    "env = gym.make('Taxi-v3', render_mode=\"rgb_array_list\")\n",
    "# env.reset()\n",
    "state = env.reset(seed=42, options={})[0]\n",
    "on_policy_MC = On_policy_MC(env=env, start_state=state, gamma=0.5)\n",
    "\n",
    "print(\"initial policy\",on_policy_MC.Pi)\n",
    "print(on_policy_MC.r_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_policy_MC.MC_learning()\n",
    "print(\"Final policy:\\n\",np.around(on_policy_MC.Pi,2))\n",
    "print(\"current qvalue: \\n\",np.around(on_policy_MC.qvalue,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_real_value = q_ana_evaluate(on_policy_MC.Pi,on_policy_MC.r_sa,on_policy_MC.P_ssa,gamma=0.8)\n",
    "# print(\"real qvalue:\\n\",np.around(q_real_value,2))\n",
    "# print(\"值函数差的范数：\\n\",np.linalg.norm(on_policy_MC.qvalue-q_real_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"访问频次\",on_policy_MC.n)\n",
    "print(\"探索因子：\",on_policy_MC.epsilon)\n",
    "# print(\"贪婪策略:\",on_policy_MC.get_greedy_policy())\n",
    "print(\"贪婪策略:\",on_policy_MC.Greedy_Pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy_action = np.argmax(on_policy_MC.Pi, axis=1)\n",
    "greedy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = []\n",
    "# state = env.reset()[0]\n",
    "action_list = []\n",
    "print(state)\n",
    "# print(state)\n",
    "# 循环交互\n",
    "while True:\n",
    "    # 按照策略选取动作\n",
    "    action = greedy_action[state]\n",
    "    print(\"state:\", state)\n",
    "    print(\"action:\", action)\n",
    "    action_list.append(action)\n",
    "    # frame.append(state)\n",
    "\n",
    "    # agent与环境进行一步交互\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # 判断当前否完成\n",
    "    if terminated:\n",
    "        print('done')\n",
    "        break\n",
    "    # time.sleep(1)\n",
    "    \n",
    "frame.append(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.array(frame)\n",
    "frames = frames.squeeze()\n",
    "len(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for i in range(frames.shape[0] - 1):\n",
    "#     plt.imshow(frames[i])\n",
    "#     # plt.title('action = {0}'.format(num_to_actual[action_list[i]]), fontsize=22)\n",
    "#     # 去除坐标轴\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # 去除周围的白边\n",
    "#     plt.tight_layout(pad=0)\n",
    "#     plt.savefig(os.path.join('./result/on_policy', 'frame_{0}.png'.format(i)))\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# def compose_gif(frame):\n",
    "#     imageio.mimsave(\"result/on_policy/on_policy.gif\", frames, duration=500)\n",
    "\n",
    "# compose_gif(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
