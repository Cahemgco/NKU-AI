{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Off_policy_MC:\n",
    "    def __init__(self, env, start_state, gamma=0.8):\n",
    "        #初始化行为值函数\n",
    "        self.state_size = env.observation_space.n\n",
    "        self.action_size = env.action_space.n\n",
    "        self.qvalue = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        #初始化每个状态-动作对的次数\n",
    "        self.C = np.zeros((self.state_size,self.action_size))\n",
    "        self.states = []    # 状态空间500\n",
    "        for i in range(self.state_size):\n",
    "            self.states.append(i)\n",
    "\n",
    "        self.actions = []   # 动作空间6\n",
    "        for i in range(self.action_size):\n",
    "            self.actions.append(i)\n",
    "        self.gamma = gamma\n",
    "        self.env = env\n",
    "        self.epsilon = 0.5\n",
    "        self.start_state = start_state\n",
    "        \n",
    "        #初始化采样策略\n",
    "        self.behaviour_Pi = (1/self.action_size) * np.ones((self.state_size, self.action_size))\n",
    "        #初始化目标策略\n",
    "        self.target_Pi = np.zeros((self.state_size,self.action_size))\n",
    "        for i in range(self.state_size):\n",
    "            action_prob = (1/self.action_size)*np.ones((1, self.action_size)).squeeze()\n",
    "            j = np.random.choice(self.actions,p=action_prob)\n",
    "            # j = np.random.choice(self.actions, p=(1/self.action_size) * np.ones(1, self.action_size))\n",
    "            self.target_Pi[i,j]=1\n",
    "        self.Greedy_Pi = np.zeros((self.state_size,self.action_size))\n",
    "        self.cur_state = 1\n",
    "        self.cur_action = 0\n",
    "        self.old_policy=np.ones((self.state_size,self.action_size))\n",
    "        #################1.状态转移概率P(s'|s,a)模型构建#################################\n",
    "        self.P_ssa = np.zeros((self.action_size, self.state_size, self.state_size))\n",
    "        self.r_sa = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "        for i in range(self.state_size):\n",
    "            for j in range(self.action_size):\n",
    "                next = self.env.P[i][j]\n",
    "                for k in range(len(next)):\n",
    "                    probability, next_state, reward, done = next[k]\n",
    "                    self.P_ssa[j][i][next_state] += probability\n",
    "                    # self.r_sa[i][j] += probability*(reward)\n",
    "                    if reward == -1:\n",
    "                        self.r_sa[i][j] += probability*(-0.1)\n",
    "                    elif reward == -10:\n",
    "                        self.r_sa[i][j] += probability*(-1)\n",
    "                    else:\n",
    "                        self.r_sa[i][j] += probability*(1000)\n",
    "\n",
    "    #重置环境函数\n",
    "    def reset(self):\n",
    "        # 初始化行为值函数\n",
    "        self.qvalue = np.zeros((self.state_size, self.action_size))\n",
    "        # 初始化每个状态-动作对的次数\n",
    "        self.C = np.zeros((self.state_size, self.action_size))\n",
    "    #根据采样策略采样一个动作\n",
    "    def sample_action(self,state):\n",
    "        action = np.random.choice(self.actions,p=self.behaviour_Pi[state,:])\n",
    "        return action\n",
    "    #跟环境交互一步\n",
    "    def step(self,action):\n",
    "        probability, next_state, reward, done = self.env.P[self.cur_state][action][0]\n",
    "        if reward == -1:\n",
    "            reward = -0.1\n",
    "        elif reward == -10:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1000\n",
    "        return next_state,reward,done\n",
    "    #############更新目标策略##########\n",
    "    def update_target_policy(self):\n",
    "        epsilon = self.epsilon/10\n",
    "        for i in range(self.state_size):\n",
    "            self.target_Pi[i, :] = epsilon / self.action_size\n",
    "            max_num = np.argmax(self.qvalue[i, :])\n",
    "            self.target_Pi[i, max_num] = epsilon / self.action_size + (1 - epsilon)\n",
    "    #############更新采样策略##########\n",
    "    def update_behaviour_policy(self):\n",
    "        for i in range(self.state_size):\n",
    "            self.behaviour_Pi[i, :] = self.epsilon / self.action_size\n",
    "            max_num = np.argmax(self.qvalue[i, :])\n",
    "            self.behaviour_Pi[i, max_num] = self.epsilon / self.action_size + (1 - self.epsilon)\n",
    "    #############获得贪婪策略##########\n",
    "    def get_greedy_policy(self):\n",
    "        for i in range(self.state_size):\n",
    "            self.Greedy_Pi[i, :] = 0\n",
    "            max_num = np.argmax(self.qvalue[i, :])\n",
    "            self.Greedy_Pi[i, max_num] = 1\n",
    "        return self.Greedy_Pi\n",
    "    #蒙特卡洛强化学习算法\n",
    "    def Off_MC_learning(self):\n",
    "        num = 0\n",
    "        self.update_target_policy()\n",
    "        self.update_behaviour_policy()\n",
    "        while num<int(20000):\n",
    "            num+=1\n",
    "            flag=False\n",
    "            #采样一条轨迹\n",
    "            state_traj=[]\n",
    "            action_traj = []\n",
    "            reward_traj=[]\n",
    "            g = 0\n",
    "            W = 1\n",
    "            episode_num = 0\n",
    "            # 从初始状态出发\n",
    "            self.cur_state = self.start_state\n",
    "            while flag==False and episode_num<50:\n",
    "                #与环境交互一次\n",
    "                cur_action = self.sample_action(self.cur_state)\n",
    "                state_traj.append(self.cur_state)\n",
    "                action_traj.append(cur_action)\n",
    "                next_state, reward, flag = self.step(cur_action)\n",
    "                reward_traj.append(reward)\n",
    "                self.cur_state = next_state\n",
    "                episode_num += 1\n",
    "            print(\"state_traj:\",state_traj)\n",
    "\n",
    "            ############利用采集到的轨迹更新行为值函数################\n",
    "            for i in reversed(range(len(state_traj))):\n",
    "                #计算状态-动作对(s,a)后的轨迹的权重和\n",
    "                # print(\"W\",W)\n",
    "                self.C[state_traj[i],action_traj[i]]+=W\n",
    "                #利用增量式方式更新当前状态动作值\n",
    "                g*=self.gamma\n",
    "                g+=reward_traj[i]\n",
    "                self.qvalue[state_traj[i],action_traj[i]]=self.qvalue[state_traj[i],action_traj[i]]+\\\n",
    "                        (W/self.C[state_traj[i],action_traj[i]])*(g-self.qvalue[state_traj[i],action_traj[i]])\n",
    "                W = W*self.target_Pi[state_traj[i],action_traj[i]]/self.behaviour_Pi[state_traj[i],action_traj[i]]\n",
    "                # print(\"W:\\n\",W)\n",
    "            ###########更新策略################\n",
    "            if num%200==0:\n",
    "                self.old_policy = copy.deepcopy(self.target_Pi)\n",
    "                self.epsilon = self.epsilon * 0.99\n",
    "                self.update_target_policy()\n",
    "                self.update_behaviour_policy()\n",
    "                self.C = np.zeros((self.state_size,self.action_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_ana_evaluate(Pi,r_sa,P_ssa):\n",
    "    state_size = 500\n",
    "    action_size = 6\n",
    "    # state_size = 16\n",
    "    # action_size = 4\n",
    "    P_pi = np.zeros((state_size, state_size))\n",
    "    C_pi = np.zeros((state_size, 1))\n",
    "    for i in range(state_size):\n",
    "        # 计算pi(a|s)*p(s'|s,a)\n",
    "        P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()\n",
    "        # 计算pi(a|s)*r(s,a)\n",
    "        C_pi[i, :] = np.dot(r_sa[i, :], Pi[i, :])\n",
    "    ############解析法计算值函数######################\n",
    "    M = np.eye(state_size) - 0.5*P_pi\n",
    "    I_M = np.linalg.inv(M)\n",
    "    V = np.dot(I_M, C_pi)\n",
    "    #计算行为值函数\n",
    "    q_value = np.zeros((state_size, action_size))\n",
    "    for i in range(state_size):\n",
    "        q_sa = np.zeros((1, action_size))\n",
    "        for j in range(action_size):\n",
    "            Pi[i, :] = 0\n",
    "            Pi[i, j] = 1\n",
    "            P_pi[i, :] = np.dot(np.expand_dims(Pi[i, :], axis=0), P_ssa[:, i, :]).squeeze()\n",
    "            vi = np.dot(r_sa[i, :], Pi[i, :]) + np.dot(P_pi[i, :], V.squeeze())\n",
    "            q_sa[0, j] = vi\n",
    "        q_value[i, :] = q_sa[0, :]\n",
    "    return q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"rgb_array_list\")\n",
    "env = gym.make('Taxi-v3', render_mode=\"rgb_array_list\")\n",
    "# env.reset()\n",
    "state = env.reset()[0]\n",
    "\n",
    "off_policy_MC = Off_policy_MC(env=env, start_state = state, gamma=0.5)\n",
    "off_policy_MC.Off_MC_learning()\n",
    "\n",
    "print(off_policy_MC.get_greedy_policy())\n",
    "print(\"估计值函数：\\n\",np.around(off_policy_MC.qvalue,2))\n",
    "print(\"Final policy:\\n\",np.around(off_policy_MC.target_Pi,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_real=q_ana_evaluate(off_policy_MC.target_Pi,off_policy_MC.r_sa,off_policy_MC.P_ssa)\n",
    "# print(\"真实值函数：\\n\",np.around(q_real,2))\n",
    "# print(\"值函数差的范数：\\n\",np.linalg.norm(off_policy_MC.qvalue-q_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"访问频次：\\n\",np.around(off_policy_MC.C,1))\n",
    "\n",
    "greedy_action = np.argmax(off_policy_MC.target_Pi, axis=1)\n",
    "greedy_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = []\n",
    "# state = env.reset()[0]\n",
    "action_list = []\n",
    "print(state)\n",
    "# print(state)\n",
    "# 循环交互\n",
    "while True:\n",
    "    # 按照策略选取动作\n",
    "    action = greedy_action[state]\n",
    "    print(\"state:\", state)\n",
    "    print(\"action:\", action)\n",
    "    action_list.append(action)\n",
    "    # frame.append(state)\n",
    "\n",
    "    # agent与环境进行一步交互\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # 判断当前否完成\n",
    "    if terminated:\n",
    "        print('done')\n",
    "        break\n",
    "    # time.sleep(1)\n",
    "    \n",
    "frame.append(env.render())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.array(frame)\n",
    "frames = frames.squeeze()\n",
    "len(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for i in range(frames.shape[0] - 1):\n",
    "    plt.imshow(frames[i])\n",
    "    # plt.title('action = {0}'.format(num_to_actual[action_list[i]]), fontsize=22)\n",
    "    # 去除坐标轴\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 去除周围的白边\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(os.path.join('./result/off_policy', 'frame_{0}.png'.format(i)))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "def compose_gif(frame):\n",
    "    imageio.mimsave(\"result/off_policy/off_policy.gif\", frames, duration=500)\n",
    "\n",
    "compose_gif(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
